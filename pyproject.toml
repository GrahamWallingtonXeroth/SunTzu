[project]
name = "suntzu"
version = "10.0.0"
description = "Turn-based strategy game API and benchmark for measuring strategic reasoning in LLM agents"
requires-python = ">=3.12"
dependencies = [
    "flask>=3.0,<4",
    "flask-cors>=4.0,<5",
    "gunicorn>=22.0,<23",
    "numpy>=2.0,<3",
    "noise>=1.2,<2",
]

[project.optional-dependencies]
dev = [
    "pytest>=8.0,<9",
    "pytest-cov>=4.1,<6",
    "pytest-flask>=1.3,<2",
    "requests>=2.31,<3",
    "ruff>=0.4",
]
llm = [
    "anthropic>=0.40,<1",
    "openai>=1.50,<2",
]

[build-system]
requires = ["setuptools>=68.0", "wheel"]
build-backend = "setuptools.build_meta"

[tool.setuptools.packages.find]
include = ["suntzu*", "benchmark*"]

[tool.pytest.ini_options]
testpaths = ["tests"]
pythonpath = ["."]

[tool.ruff]
target-version = "py312"
line-length = 120

[tool.ruff.lint]
select = [
    "E",     # pycodestyle errors
    "W",     # pycodestyle warnings
    "F",     # pyflakes
    "I",     # isort
    "UP",    # pyupgrade
    "B",     # flake8-bugbear
    "SIM",   # flake8-simplify
    "RUF",   # ruff-specific rules
]
ignore = [
    "E501",  # line length handled by formatter
    "SIM108", # ternary operator suggestions
]

[tool.ruff.lint.isort]
known-first-party = ["models", "state", "orders", "resolution", "upkeep", "map_gen", "benchmark"]

[tool.coverage.run]
source = ["."]
omit = ["tests/*", "benchmark/*"]

[tool.coverage.report]
show_missing = true
skip_empty = true
